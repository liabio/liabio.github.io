<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on 码农实战</title>
    <link>http://www.liabio.cn/tags/kubernetes/</link>
    <description>Recent content in kubernetes on 码农实战</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 11 Oct 2019 21:21:50 +0800</lastBuildDate>
    
	<atom:link href="http://www.liabio.cn/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>kubernetes垃圾回收器Garbage Collector Controller源码分析（二）</title>
      <link>http://www.liabio.cn/posts/2019-10-11-kubernetes%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8GarbageCollectorController%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902/</link>
      <pubDate>Fri, 11 Oct 2019 21:21:50 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-10-11-kubernetes%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8GarbageCollectorController%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902/</guid>
      <description>kubernetes版本：1.13.2 接上一节：kubernetes垃圾回收器GarbageCollector Controller源码分析（一） 主要步骤 GarbageCollector Controller源码主要分为以下几部分： monitors作为生产者将变化的资源放入</description>
    </item>
    
    <item>
      <title>如何批量删除k8s资源对象</title>
      <link>http://www.liabio.cn/posts/2019-10-10-%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4k8s-pvc-pv/</link>
      <pubDate>Thu, 10 Oct 2019 17:20:28 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-10-10-%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4k8s-pvc-pv/</guid>
      <description>在云平台开发、中间件容器化时，经常会遇到批量删除k8s资源对象的需求，下面记录一下kubectl和golang发送删除pvc、pv、pod请求的例子，便于后续学习查阅 kubectl发送删除请求 根据label批量删除pod： kubectl delete pod -n kube-system -l &amp;quot;harmonycloud.cn/statefulset=redis-ll-1010-a&amp;quot; 根据</description>
    </item>
    
    <item>
      <title>kubernetes自定义资源对象高级功能</title>
      <link>http://www.liabio.cn/posts/2019-09-30-kubernetes%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/</link>
      <pubDate>Mon, 30 Sep 2019 14:41:33 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-09-30-kubernetes%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/</guid>
      <description>kubernetes自定义资源对象再极大程度提高了API Server的可扩展性，让企业能够根据业务需求通过CRD编写controller或者operator来实现生产中各种特殊场景。随着k8s的版本升级，CRD的功能也越来越完善，下面对其中</description>
    </item>
    
    <item>
      <title>etcd操作</title>
      <link>http://www.liabio.cn/posts/2019-5-21-etcd%E6%93%8D%E4%BD%9C/</link>
      <pubDate>Fri, 27 Sep 2019 09:59:46 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-5-21-etcd%E6%93%8D%E4%BD%9C/</guid>
      <description>etcdctl安装 下载并解压二进制文件 curl -L https://github.com/coreos/etcd/releases/download/v3.3.2/etcd-v3.3.2-linux-amd64.tar.gz -o etcd-v3.3.2-linux-amd64.tar.gz tar zxf etcd-v3.3.2-linux-amd64.tar.gz 解压后是一些文档和两个二进制文件etcd和etcdctl。etcd是server端，etcdctl是客户端。 将解压后的etcd和etcdctl移动到$GOPATH/bin目录下，可以直接</description>
    </item>
    
    <item>
      <title>k8s中部署负载均衡器ingress-nginx</title>
      <link>http://www.liabio.cn/posts/2019-09-24-k8s%E4%B8%AD%E9%83%A8%E7%BD%B2ingress-nginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8/</link>
      <pubDate>Tue, 24 Sep 2019 09:59:46 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-09-24-k8s%E4%B8%AD%E9%83%A8%E7%BD%B2ingress-nginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8/</guid>
      <description>正文 在Kubernetes中，服务和Pod的IP地址仅可以在集群网络内部使用，对于集群外的应用是不可见的。为了使外部的应用能够访问集群内的服务，在Kubernetes 目前 提供了以下几种方案： NodePort LoadBalancer Ingress 本节主要就ingress和ingress控制</description>
    </item>
    
    <item>
      <title>k8s使用Job执行任务失败了怎么办</title>
      <link>http://www.liabio.cn/posts/2019-09-24-k8s%E4%BD%BF%E7%94%A8Job%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1%E5%A4%B1%E8%B4%A5%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/</link>
      <pubDate>Tue, 24 Sep 2019 09:59:46 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-09-24-k8s%E4%BD%BF%E7%94%A8Job%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1%E5%A4%B1%E8%B4%A5%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E/</guid>
      <description>正文 Kubernetes 中使用 Job 和 CronJob 两个资源分别提供了一次性任务和定时任务的特性，这两种对象也使用控制器模型来实现资源的管理，我们在这篇文章来介绍Job执行如果失败了会怎么样呢？ 修改job-fail.yaml，故意引入一个错误： Never 如果将 restartPolicy 设置为 Never 会怎么样？下</description>
    </item>
    
    <item>
      <title>kubernetes垃圾回收器GarbageCollector Controller源码分析（一）</title>
      <link>http://www.liabio.cn/posts/2019-09-24-kubernetes%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8GarbageCollectorController%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901/</link>
      <pubDate>Tue, 24 Sep 2019 09:59:46 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-09-24-kubernetes%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8GarbageCollectorController%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901/</guid>
      <description>kubernetes版本：1.13.2 背景 由于operator创建的redis集群，在kubernetes apiserver重启后，redis集群被异常删除（包括redis exporter statefulset、redis statefulset）。删除后</description>
    </item>
    
    <item>
      <title>采坑指南——k8s域名解析coredns问题排查过程</title>
      <link>http://www.liabio.cn/posts/2019-09-24-%E9%87%87%E5%9D%91%E6%8C%87%E5%8D%97-k8s%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90coredns%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B/</link>
      <pubDate>Tue, 24 Sep 2019 09:59:46 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-09-24-%E9%87%87%E5%9D%91%E6%8C%87%E5%8D%97-k8s%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90coredns%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B/</guid>
      <description>正文 前几天，在ucloud上搭建的k8s集群（搭建教程后续会发出）。今天发现域名解析不了。 组件版本：k8s 1.15.0，coredns：1.3.1 过程是这样的： 首先用以下yaml文件创建了一个nginx服务 apiVersion: v1 kind: Service metadata: name: nginx-svc-old labels: app: nginx-svc spec: selector: app: nginx ports: - protocol:</description>
    </item>
    
    <item>
      <title>手把手教你搭建kubernetes集群</title>
      <link>http://www.liabio.cn/posts/2019-05-14-%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Tue, 14 May 2019 09:59:46 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-05-14-%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A4/</guid>
      <description>cat &amp;gt; /etc/yum.repos.d/kubernetes.repo &amp;lt;&amp;lt; EOF [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=0 repo_gpgcheck=1 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 安装kubeadm、kubectl、kubelet yum install -y kubeadm kubelet kubectl 如果在云服务器上搭建时，IP-18.219.28.143是公网IP kubeadm init --kubernetes-version=v1.14.1 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --apiserver-advertise-address=18.219.28.143 --ignore-preflight-errors=Swap,NumCPU init的时候可能会： [kubelet-check] Initial timeout of 40s passed ![](/img/build-kubernetes-cluster-learning/install_kubernetes_cluster2.png) 需要把/etc/kubernete</description>
    </item>
    
    <item>
      <title>手把手教你搭建kubernetes集群1</title>
      <link>http://www.liabio.cn/posts/2019-05-14-%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A41/</link>
      <pubDate>Tue, 14 May 2019 09:59:46 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-05-14-%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E6%90%AD%E5%BB%BAkubernetes%E9%9B%86%E7%BE%A41/</guid>
      <description>部署 以CentOS7为基础，搭建一个Master主机和三个Node主机，各个Node主机的配置方式基本相同。 OS: CentOS 7.5 x86_64 Container runtime: Docker 18.06.ce Kubernetes: 1.13 IP 地址 主机名 角色 192.168.50.71 master, master.kubernetes.io master 192.168.50.72 node01, node01.kubernetes.io node 192.168.50.73 node02, node02.kubernetes.io node 192.168.50.74 node03, node03.kubernetes.io node 这里需要使用常规的域名格式，因为后面需要为集群配置Kuberne</description>
    </item>
    
    <item>
      <title>搭建k8s环境时gcr.io和quay.io拉取镜像失败</title>
      <link>http://www.liabio.cn/posts/2019-05-14-%E6%90%AD%E5%BB%BAk8s%E7%8E%AF%E5%A2%83%E6%97%B6gcr.io%E5%92%8Cquay.io%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E5%A4%B1%E8%B4%A5/</link>
      <pubDate>Tue, 14 May 2019 09:59:46 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-05-14-%E6%90%AD%E5%BB%BAk8s%E7%8E%AF%E5%A2%83%E6%97%B6gcr.io%E5%92%8Cquay.io%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E5%A4%B1%E8%B4%A5/</guid>
      <description>k8s在使用编排（manifest）工具进行yaml文件启动pod时，会遇到官方所给例子中spec.containers.image包含： quay.io/coreos/example_ gcr.io/google_containers/example_ 也就是说，从quay.io和gcr.io进行镜像拉取，我们知道，国内访问外网是被屏蔽了的。可以将其</description>
    </item>
    
    <item>
      <title>kube-scheduler调度扩展</title>
      <link>http://www.liabio.cn/posts/2019-05-01-kube-scheduler%E8%B0%83%E5%BA%A6%E6%89%A9%E5%B1%95/</link>
      <pubDate>Wed, 01 May 2019 09:59:46 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2019-05-01-kube-scheduler%E8%B0%83%E5%BA%A6%E6%89%A9%E5%B1%95/</guid>
      <description>正文 Kubernetes 自带了一个默认调度器kube-scheduler，其内置了很多节点预选和优选的调度算法，一般调度场景下可以满足要求。但是在一些特殊场景下，默认调度器不能满足我们复杂的调度需求。我们就需要对调度器进行扩展，以达到调度适合业务场景的目的。</description>
    </item>
    
    <item>
      <title>Pod调度到集群中某node节点失败</title>
      <link>http://www.liabio.cn/posts/2018-12-13-pod%E8%B0%83%E5%BA%A6%E5%88%B0%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%9F%90node%E8%8A%82%E7%82%B9%E5%A4%B1%E8%B4%A5/</link>
      <pubDate>Thu, 13 Dec 2018 16:43:35 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2018-12-13-pod%E8%B0%83%E5%BA%A6%E5%88%B0%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%9F%90node%E8%8A%82%E7%82%B9%E5%A4%B1%E8%B4%A5/</guid>
      <description>Fixing rpc.statd is not running but is required for remote locking If you come across this error while attempting to mount an NFS filesystem it means that the statd process is not running. mount -a -t nfs mount.nfs: rpc.statd is not running but is required for remote locking. mount.nfs: Either use &#39;-o nolock&#39; to keep locks local, or start statd. mount.nfs: an incorrect mount option was specified Here’s how to fix the rpc.statd is not running error on el6 First, ensure that rpcbind is running and that it is set to start on boot /etc/init.d/rpcbind start Starting rpcbind: [ OK ] chkconfig rpcbind on Then, start the nfslock service /etc/init.d/nfslock start Starting NFS statd: [</description>
    </item>
    
    <item>
      <title>搭建k8s集群遇到的问题</title>
      <link>http://www.liabio.cn/posts/2018-11-14-%E6%90%AD%E5%BB%BAk8s%E9%9B%86%E7%BE%A4%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Wed, 14 Nov 2018 16:57:38 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2018-11-14-%E6%90%AD%E5%BB%BAk8s%E9%9B%86%E7%BE%A4%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>部署CalicoNode（所有节点）报错： Oct 10 21:04:55 smallsoup docker[76907]: bird: BGP: Unexpected connect from unknown address 192.168.1.104 (port 58153) Oct 10 21:04:59 smallsoup docker[76907]: bird: BGP: Unexpected connect from unknown address 192.168.1.104 (port 40415) Oct 10 21:05:00 smallsoup docker[76907]: 2018-10-10 13:05:00.213 [INFO][87] int_dataplane.go 690: Applying dataplane updates Oct 10 21:05:00 smallsoup docker[76907]: 2018-10-10 13:05:00.213 [INFO][87] ipsets.go 224: Asked to resync with the dataplane on next update. family=&amp;quot;inet&amp;quot; Oct 10 21:05:00 smallsoup docker[76907]: 2018-10-10 13:05:00.213 [INFO][87] ipsets.go 255: Resyncing ipsets with dataplane. family=&amp;quot;inet&amp;quot; Oct 10 21:05:00 smallsoup docker[76907]: 2018-10-10 13:05:00.218 [INFO][87] ipsets.go 297: Finished resync family=&amp;quot;inet&amp;quot; numInconsistenciesFound=0 resyncDuration=4.542824ms Oct 10 21:05:00 smallsoup docker[76907]: 2018-10-10</description>
    </item>
    
    <item>
      <title>Kubernetes Pod Exec接口调用</title>
      <link>http://www.liabio.cn/posts/2018-10-02-kubernetesPodExec%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8/</link>
      <pubDate>Tue, 02 Oct 2018 15:12:59 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2018-10-02-kubernetesPodExec%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8/</guid>
      <description>一般生产环境上由于网络安全策略，大多数端口是不能为集群外部访问的。多个集群之间一般都是通过k8s的ApiServer组件提供的接口通信，如https://192.168.1.101:6443。所以在做云平台时，集群管理平台（雅称：观云台）需</description>
    </item>
    
    <item>
      <title>史上最全k8s必学必会知识梳理</title>
      <link>http://www.liabio.cn/posts/2018-03-11-%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8k8s%E5%BF%85%E5%AD%A6%E5%BF%85%E4%BC%9A%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/</link>
      <pubDate>Sun, 11 Mar 2018 14:56:02 +0800</pubDate>
      
      <guid>http://www.liabio.cn/posts/2018-03-11-%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8k8s%E5%BF%85%E5%AD%A6%E5%BF%85%E4%BC%9A%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/</guid>
      <description>正文 kube-apiserver 对外暴露了Kubernetes API。它是的 Kubernetes 核心控制层。它被设计为水平扩展，即通过部署更多实例来横向扩展。API Server 负责和 etcd 交互（其他组件不会直接操作 etcd，只有 API Server 这么做），是整个 kubernetes 集群的数据中心，所有的交互都是以 API Server 为核心的。A</description>
    </item>
    
  </channel>
</rss>